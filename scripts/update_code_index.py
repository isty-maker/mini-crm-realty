#!/usr/bin/env python3
"""Regenerate docs/code-index.md with RAW GitHub links for every repo file."""
from __future__ import annotations

import argparse
import os
from collections import defaultdict
from pathlib import Path
from typing import Dict, Iterable, List
from urllib.error import HTTPError, URLError
from urllib.request import Request, urlopen

BASE_RAW_URL = "https://raw.githubusercontent.com/isty-maker/mini-crm-realty/refs/heads/main/"
REPO_ROOT = Path(__file__).resolve().parents[1]
DOC_PATH = REPO_ROOT / "docs" / "code-index.md"

EXCLUDE_DIR_NAMES = {
    ".git",
    "__pycache__",
    ".pytest_cache",
    ".mypy_cache",
    ".venv",
    "venv",
    "node_modules",
}
EXCLUDE_DIR_PATHS = {
    Path(".github") / "workflows",
}


def should_skip_dir(root: Path, current: Path) -> bool:
    """Return True if the directory should be excluded from the index."""
    rel = current.relative_to(root)
    if rel in EXCLUDE_DIR_PATHS:
        return True
    return any(part in EXCLUDE_DIR_NAMES for part in rel.parts)


def iter_repo_files(root: Path) -> List[Path]:
    files: List[Path] = []
    for dirpath, dirnames, filenames in os.walk(root):
        dirpath_path = Path(dirpath)
        rel_dir = dirpath_path.relative_to(root)
        # Filter directories in-place for os.walk
        filtered_dirnames = []
        for name in sorted(dirnames):
            candidate = dirpath_path / name
            if should_skip_dir(root, candidate):
                continue
            filtered_dirnames.append(name)
        dirnames[:] = filtered_dirnames

        for filename in sorted(filenames):
            file_path = dirpath_path / filename
            rel_path = file_path.relative_to(root)
            # Skip files under excluded directories even if os.walk reached them
            if any(part in EXCLUDE_DIR_NAMES for part in rel_path.parts):
                continue
            if any(rel_path.is_relative_to(p) for p in EXCLUDE_DIR_PATHS):  # type: ignore[attr-defined]
                # Python < 3.9 compatibility fallback handled below
                continue
            files.append(rel_path)
    # Ensure deterministic order
    files.sort(key=lambda p: p.as_posix())
    return files


def is_under(path: Path, ancestor: Path) -> bool:
    try:
        path.relative_to(ancestor)
        return True
    except ValueError:
        return False


# Compatibility fallback for Python < 3.9 where Path.is_relative_to does not exist
if not hasattr(Path, "is_relative_to"):
    def _is_relative_to(self: Path, other: Path) -> bool:
        return is_under(self, other)

    Path.is_relative_to = _is_relative_to  # type: ignore[attr-defined]


def group_files(paths: Iterable[Path]) -> Dict[str, List[str]]:
    grouped: Dict[str, List[str]] = defaultdict(list)
    for path in paths:
        parent = path.parent.as_posix()
        section = "/" if parent == "." else parent
        grouped[section].append(path.as_posix())
    for files in grouped.values():
        files.sort()
    return dict(sorted(grouped.items(), key=lambda item: item[0]))


def validate_links(urls: Iterable[str], *, timeout: float = 10.0) -> None:
    offline = False
    for url in urls:
        try:
            status = fetch_status(url, timeout=timeout)
        except URLError as exc:
            print(f"[warn] Unable to reach {url}: {exc}. Assuming offline mode, skipping validation.")
            offline = True
            break
        except HTTPError as exc:
            if exc.code == 405:
                status = fetch_status(url, method="GET", timeout=timeout)
            else:
                raise RuntimeError(f"Link validation failed for {url}: HTTP {exc.code}") from exc
        if status != 200:
            raise RuntimeError(f"Link validation failed for {url}: HTTP {status}")
    if offline:
        print("[warn] Link validation skipped for remaining files due to connectivity issues.")


def fetch_status(url: str, *, method: str = "HEAD", timeout: float = 10.0) -> int:
    request = Request(url, method=method, headers={"User-Agent": "code-index-updater/1.0"})
    with urlopen(request, timeout=timeout) as response:  # type: ignore[arg-type]
        return response.status


def build_document(grouped: Dict[str, List[str]]) -> str:
    lines = ["# Repository Code Index", "", "_Generated by scripts/update_code_index.py_", ""]
    for section, files in grouped.items():
        lines.append(f"## {section}")
        lines.append("")
        for path in files:
            lines.append(f"[{path}]")
            lines.append(f"RAW: {BASE_RAW_URL}{path}")
            lines.append("")
    return "\n".join(lines).rstrip() + "\n"


def main() -> None:
    parser = argparse.ArgumentParser(description="Regenerate docs/code-index.md with RAW links")
    parser.add_argument(
        "--skip-validation",
        action="store_true",
        help="Skip HTTP validation of generated RAW links",
    )
    args = parser.parse_args()

    files = iter_repo_files(REPO_ROOT)
    grouped = group_files(files)
    document = build_document(grouped)
    DOC_PATH.write_text(document, encoding="utf-8")
    print(f"[info] Wrote index for {len(files)} files to {DOC_PATH.relative_to(REPO_ROOT)}")

    if not args.skip_validation:
        try:
            validate_links(BASE_RAW_URL + path.as_posix() for path in files)
        except RuntimeError as exc:
            raise SystemExit(str(exc))


if __name__ == "__main__":
    main()
